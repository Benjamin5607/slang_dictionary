import os
import random # ëœë¤ ë½‘ê¸°ìš©
from flask import Flask, request, jsonify, send_file
import requests
from bs4 import BeautifulSoup
import urllib.parse

app = Flask(__name__)

# ---------------------------------------------------------
# ğŸ­ ì¸ê°„ ì½”ìŠ¤í”„ë ˆìš© ê°€ë©´ (User-Agent ë¦¬ìŠ¤íŠ¸)
# ---------------------------------------------------------
USER_AGENTS = [
    # Chrome (Windows)
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    # Edge (Windows)
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    # Whale (Naver Browser) - ë„¤ì´ë²„ ëš«ì„ ë•Œ íš¨ê³¼ì 
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Whale/3.23.214.17 Safari/537.36",
    # Safari (Mac)
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
]

def get_headers(referer_url):
    """
    ì§„ì§œ ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ëŠ” í—¤ë”ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    return {
        'User-Agent': random.choice(USER_AGENTS), # ê°€ë©´ ëœë¤ ì°©ìš©
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7', # í•œêµ­ì¸ì¸ ì²™
        'Referer': referer_url, # "ë‚˜ ë„¤ì´ë²„ ë©”ì¸ì—ì„œ ê²€ìƒ‰í•´ì„œ ë“¤ì–´ì˜¨ê±°ì•¼"ë¼ê³  ë»¥ì¹˜ê¸°
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }

@app.route('/')
def home():
    try:
        return send_file('index.html')
    except Exception as e:
        return str(e)

# ---------------------------------------------------------
# â›ï¸ ë§ŒëŠ¥ ì±„êµ´ê¸° (DuckDuckGo)
# ---------------------------------------------------------
def scrape_ddg(term, country_code):
    # êµ­ê°€ë³„ ì½”ë“œ ë§¤í•‘
    regions = {'KR': 'kr-kr', 'JP': 'jp-jp', 'CN': 'cn-zh', 'VN': 'vn-vi', 
               'FR': 'fr-fr', 'DE': 'de-de', 'ES': 'es-es', 'RU': 'ru-ru', 
               'BR': 'br-pt', 'MX': 'mx-es'}
    region = regions.get(country_code, 'wt-wt')

    suffix = "meaning"
    if country_code == 'KR': suffix = "ëœ» ì˜ë¯¸"
    if country_code == 'JP': suffix = "ã¨ã¯ æ„å‘³"
    
    url = "https://lite.duckduckgo.com/lite/"
    payload = {'q': f"{term} {suffix}", 'kl': region}
    
    # ì—¬ê¸°ì„œë„ ì‚¬ëŒì¸ ì²™!
    headers = get_headers('https://duckduckgo.com/')
    
    results = []
    try:
        res = requests.post(url, data=payload, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('table:nth-of-type(3) tr')
        
        current_title = ""
        for row in rows:
            link_tag = row.select_one('.result-link')
            if link_tag:
                current_title = link_tag.get_text(strip=True)
                current_link = link_tag['href']
                continue
            
            snippet_tag = row.select_one('.result-snippet')
            if snippet_tag and current_title:
                results.append({
                    'word': current_title,
                    'definition': snippet_tag.get_text(strip=True),
                    'example': current_link,
                    'thumbs_up': 'Web Search'
                })
                current_title = ""
                if len(results) >= 4: break
    except Exception as e:
        print(f"DDG Error: {e}")
        
    return results

# ---------------------------------------------------------
# ğŸš€ ë©”ì¸ ë¼ìš°í„°
# ---------------------------------------------------------
@app.route('/scrape')
def scrape():
    term = request.args.get('term')
    country = request.args.get('country')
    
    if not term: return jsonify({'error': 'No term'})

    print(f"ğŸ•µï¸ Human-like Request: {country} - {term}")
    data_list = []

    try:
        # 1. ğŸ‡°ğŸ‡· í•œêµ­ (ë„¤ì´ë²„ ì˜¤í”ˆì‚¬ì „)
        if country == 'KR':
            try:
                # ë„¤ì´ë²„ëŠ” Referer(ì´ì „ ì£¼ì†Œ)ë¥¼ ì²´í¬í•˜ë¯€ë¡œ ê¼­ ë„£ì–´ì¤˜ì•¼ í•¨
                base_url = "https://dict.naver.com/"
                search_url = f"https://dict.naver.com/search.dict?dicQuery={urllib.parse.quote(term)}&query={urllib.parse.quote(term)}&target=dict&ie=utf8&query_utf=&isOnlyViewEE="
                
                # â˜… í•µì‹¬: ë„¤ì´ë²„ ì „ìš© ê°€ë©´ ì°©ìš©
                headers = get_headers(base_url)
                
                res = requests.get(search_url, headers=headers, timeout=3)
                soup = BeautifulSoup(res.text, 'html.parser')
                
                # ì„ íƒì ìˆ˜ì • (ë„¤ì´ë²„ê°€ HTMLì„ ìì£¼ ë°”ê¿”ì„œ ë„“ê²Œ ì¡ìŒ)
                items = soup.select('.search_list li')
                
                for item in items[:4]:
                    dt = item.select_one('dt')
                    dd = item.select_one('dd')
                    
                    if dt and dd:
                        word_text = dt.get_text(strip=True)
                        # ê²€ìƒ‰ì–´ë‘ ë¹„ìŠ·í•œ ê²ƒë§Œ ê°€ì ¸ì˜¤ê¸°
                        if term in word_text or word_text in term:
                            link_tag = dt.select_one('a')
                            link = "https://dict.naver.com/" + link_tag['href'] if link_tag else "#"
                            
                            data_list.append({
                                'word': word_text,
                                'definition': dd.get_text(strip=True),
                                'example': link,
                                'thumbs_up': 'Naver Dict'
                            })
            except Exception as e:
                print(f"Naver Fail: {e}")
            
            # ì‹¤íŒ¨í•˜ë©´ ë•ë•ê³  íˆ¬ì…
            if not data_list: data_list = scrape_ddg(term, 'KR')

        # 2. ğŸ‡¯ğŸ‡µ ì¼ë³¸ (Weblio)
        elif country == 'JP':
            try:
                url = f"https://www.weblio.jp/content/{urllib.parse.quote(term)}"
                headers = get_headers('https://www.google.co.jp/')
                
                res = requests.get(url, headers=headers, timeout=3)
                soup = BeautifulSoup(res.text, 'html.parser')
                
                title = soup.select_one('.midashigo')
                desc = soup.select_one('.kiji')
                
                if title and desc:
                    data_list.append({
                        'word': title.get_text(strip=True),
                        'definition': desc.get_text(strip=True)[:150] + "...",
                        'example': url,
                        'thumbs_up': 'Weblio'
                    })
            except Exception as e:
                print(f"Weblio Fail: {e}")

            if not data_list: data_list = scrape_ddg(term, 'JP')

        # 3. ğŸ‡ºğŸ‡¸ ì˜ë¯¸ê¶Œ (Urban Dictionary API)
        elif country in ['US', 'GB', 'AU', 'CA']:
            try:
                # API í˜¸ì¶œí•  ë•Œë„ í—¤ë” ë„£ìœ¼ë©´ ë” ì•ˆì „í•¨
                headers = get_headers('https://www.urbandictionary.com/')
                res = requests.get(f"https://api.urbandictionary.com/v0/define?term={term}", headers=headers, timeout=5)
                data_list = res.json().get('list', [])
            except:
                pass

        # 4. ê·¸ ì™¸ êµ­ê°€
        else:
            data_list = scrape_ddg(term, country)

    except Exception as e:
        return jsonify({'error': str(e)})

    return jsonify({'mode': 'SCRAPE', 'list': data_list})

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 8080))
    app.run(host='0.0.0.0', port=port)
